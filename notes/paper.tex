\documentclass[10pt, conference]{IEEEtran}

\usepackage[normalem]{ulem}
\usepackage{listings}
\usepackage{array}
\usepackage{float}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage[numbers, sort&compress]{natbib}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{clrscode3e}
\usepackage{dcolumn}
\usepackage{nonfloat}
\renewcommand{\baselinestretch}{1.1}
\newcolumntype{d}{D{.}{.}{2.5}}
\newcolumntype{s}{D{.}{.}{1.2}}
\crefname{section}{}{}
\crefname{subsection}{}{}
\floatname{algorithm}{Algorithm}


\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}
\renewcommand{\bibfont}{\footnotesize}

\makeatother

\begin{document}

\title{A Memory-access Efficient Parallel Algorithm for Spectrum-based Short Read Error Correction}

\author{\IEEEauthorblockN{Nagakishore Jammula\IEEEauthorrefmark{3}\IEEEauthorrefmark{1}, Sriram Chockalingam\IEEEauthorrefmark{3}\IEEEauthorrefmark{2}, and Srinivas Aluru\IEEEauthorrefmark{1}}
\IEEEauthorblockA{
\IEEEauthorrefmark{1}Georgia Institute of Technology, Atlanta, Georgia, USA }
\IEEEauthorblockA{
\IEEEauthorrefmark{2}Indian Institute of Technology Bombay, Mumbai, India}
\IEEEauthorblockA{
\IEEEauthorrefmark{3}(Student Authors)}
}


\maketitle

\begin{abstract}
DNA read error correction enhances the quality of results produced by applications such as genome assembly, metagenomics, and transcriptomics. Use of error corrected reads also improves the runtime and memory usage of such applications. Sequential error correction tools cannot cope with billions of reads produced by the modern day sequencing instruments. A scalable parallel spectrum-based error correction algorithm was proposed to overcome this shortcoming. In this work, we improved the memory-access efficiency of the error correction phase, which is the computationally most intensive step of the algorithm, by up to 2.4x. We expect that even better speedup can be obtained for large datasets, such as those generated for humans and plants.
\end{abstract}

%\begin{IEEEkeywords}
%todo1; todo2; todo3; todo4;
%\end{IEEEkeywords}

\section{Introduction} \label{sec_Introduction}
Genome of an organism can be interpreted as a string over a four letter alphabet -- A, C, G, and T. Depending upon the organism, the length of the genome can range from several million characters to several billion characters. High-throughput Sequencing, which involves reading the constituent characters of a specific region of interest of a genome, has become an important tool in genomics research. The sequencing instruments in vogue today can only read short substrings of the genome. However, these instruments can read in excess of a billion such substrings at time, thus covering the entire length of genome many times over. These substrings, of few hundred bases length, are commonly referred to as {\it reads}. Note that each base of the genome can spanned by multiple reads. This oversampling is required to facilitate subsequent analysis of the reads. {\it Coverage} is defined as the average number of reads covering a character in the DNA.

Owing to the limitations of the sequencing technology, these {\it reads} contain errors. The nature of read errors depends upon the type of the instrument. While the predominant error type in case of Illumina sequencing instruments is substitutions, it is insertions/deletions in case of instruments from vendors such as Ion Torrent and 454/Roche. Since Illumina machines are widely used, we focus on correcting substitution errors. Read error correction is extremely important due to two primary reasons. One, it improves the quality of results in downstream applications such as genome assembly, metagenomics, and transcriptomics. Two, error correction leads to reduction in runtime and memory usage of such applications. 

Illumina instruments have error rates in the range of 1-2\%. Combining this characteristic with the observation that each character in the genome is oversampled, many methods have been introduced to correct read errors. Spectrum-based error correction is a popular paradigm to correct substitution errors and it comprises of two major steps. The first step is to construct the kmer-spectrum, the list of all substrings of length $k$, from the set of reads. In the second step, read errors are corrected by querying the kmer-spectrum. Owing to the large size of the read datasets and the computationally intensive nature of error correction, there is an acute need for parallel error correction algorithms. Recently, Shah et al. proposed a scalable parallel algorithm for {\it kmer} for spectrum-based short read error correction \cite{shah2012parallel}. The solution proposed by Shah et al. involves a series of binary searches over a sorted list of kmers making up the kmer-spectrum. In this work, we make a case for adopting cache-oblivious and cache-aware search trees to significantly enhance the performance of this step. Our results demonstrate that the runtime of the second step could be improved by up to 2.4x by using cache-oblivious and cache-aware search trees, which are optimized for memory accesses.

The rest of the paper is organized as follows. We describe the workings of the parallel spectrum-based error correction algorithm in Section \cref{sec_Parallel}. Our solution approach for constructing and accessing the memory-access efficient search trees is presented in Section \cref{sec_Memory-access}. Experimental methodology is explained in Section \cref{sec_Methodology} and the corresponding results are furnished in Section \cref{sec_Results}. In Section \cref{sec_Discussion}, we analyze the reasons for the discrepancy between the expected and actual performance of cache-oblivious and cache-aware search trees. Finally, our conclusions are provided in Section \cref{sec_Conclusion}.


\section{Parallel Spectrum-based Error Correction} \label{sec_Parallel}
kmer-spectrum based error correction methods, are mainly targeted towards correcting substitution errors. Each read of length $n$ is decomposed into $(n-k+1)$ sub-reads called {\it kmers} by reading substrings of length {\it k} starting at each position. Once the pool of kmers is generated, each kmer is classified as either valid or invalid. Valid kmers are those that actually occur in the genome while invalid kmers are those that do not. The latter originate because of sequencing errors. Since it is not possible to distinguish valid kmers from invalid ones, the following heuristic is used to categorize kmers. A kmer which occurs more number of times than a threshold value is labelled as valid. This threshold value is influenced by the coverage of the genome. The remaining kmers are marked as invalid. The objective of the error correction process is to convert invalid kmers into valid kmers with a minimum number of substitution operations. The correction choices for invalid kmers are generated by examining the valid neighbors using the hamming distance metric.

While the specific details of error correction vary among different algorithms, all of them are comprised of two important and computationally intensive steps. The first step involves constructing the kmer-spectrum, which represents the set of valid kmers. In the second step, read errors are corrected by making use of the kmer-spectrum. Leveraging on this observation, Shah et al. \cite{shah2012parallel} proposed a generic spectrum-based error correction framework for implementation on distributed memory parallel computers. The parallel algorithm is composed of the following stages: (1) reads are evenly distributed among the processors (2) each processor generates an unordered multiset of kmers (represented as integers) based on the read set allocated to it (3) each processor sorts its local list of kmers to eliminate duplicates and obtain frequencies (4) the entire list of kmers across all processors is globally sorted using a parallel sample sort (5) kmer-spectrum comprising of only valid kmers is built (6) the kmer-spectrum is copied on all processors (7) error correction is performed independently on each processor for the reads assigned to it. The implementation was demonstrated to scale well with the number of processors, thus enabling error correction for very large data sets.

Step (7) is the most computationally intensive step in the parallel algorithm and accounts for almost all the runtime in case of large data sets. A core operation in this step involves performing repeated binary searches on the kmer-spectrum in order to determine whether a kmer is valid or invalid. However, performing binary search on a sorted list laid out linearly is inefficient in terms of memory accesses. The memory access efficiency can be significantly improved by using cache-oblivious and cache-aware search trees. We describe our approach for constructing and accessing these search trees in the following section.


\section{Memory-access Efficient Search Trees} \label{sec_Memory-access}

\begin{algorithm}
\caption{Cache Aware Search Tree Construction\label{alg:caware}}
\begin{codebox}
\Procname{$\proc{Cache-Aware-Search-Tree}(B, X)$}
\zi \kw{Input :} $B$, Cache line capacity ($B \geq 3$).
\zi \kw{Input :} $X$, Sorted List. $N \gets |X|$ ($N \bmod{B}$ is $0$).
\zi \kw{Output :} $CT$, Cache aware memory layout of $X$.
\li $L \gets \lceil \log_{B+1} (N+1) \rceil$; $i \gets 0$.
\li $CT[0] \gets 0$; $CT[1] \gets L$; $CT[B-1] \gets N-1$
\li $l_{ptr} \gets 0$ \Comment Pointer to last inserted node.
\li $c_{ptr} \gets 0$ \Comment Pointer to current node.
\li \While $i < n$
    \Do
\zi      \Comment $[x,y]$ range covered by this subtree.
\li      $x \gets CT[c_{ptr}]$; \quad $y \gets CT[c_{ptr}+B-1]$
\zi      \Comment $k$ no. of levels of current subtree.
\li      $k \gets CT[c_{ptr} + 1]$; \quad $d \gets y - x + 1$
\li      $ST \gets \proc{Subtree-Size}(y - x + 1, k)$
\li      $y \gets x$
\li      \For $j \gets 0 \To B - 1$ \Comment Current node.
         \Do
\li           $y \gets y + ST[j]$
\li           $CT[c_{ptr} + j] \gets X[y]$
\li           $y = y + 1$
         \End
\li      \For $j \gets 0 \To B$ \Comment Sub-tree roots.
         \Do
\li           \If $ST[j] > 0$
              \Do
\li               $y \gets x + ST[j]$
\li               $l_{ptr} \gets l_{ptr} + B$
\li               $CT[l_{ptr}] \gets x$
\li               $CT[l_{ptr} + 1] \gets k - 1$
\li               $CT[l_{ptr} + B - 1] \gets y - 1$
\li               $x \gets y + 1$
              \End
        \End
\li     $i = i + B$; $c_{ptr} = c_{ptr} + B$
    \End
\li \Return $CT$
\end{codebox}
\begin{codebox}
\Procname{$\proc{Subtree-Size}(d,k)$}
\zi \kw{Input : } $d$, Tree size; $k$, Tree levels
\zi \kw{Output : }  $ST$, $B+1$ subtree sizes
\zi Global array $A_1[i] = B(B+1)^{i}, 0 \leq i < l$
\zi Global array $A_2[i] = \sum_{j = 0}^i A_1[j], 0 \leq i < l$
\li \If $d \isequal B$ \Comment Last row : All zeroes
\li \Then \Return $ST$ with $B+1$ zeros. \End
\li $d_k \gets d - A_2[k - 2]$
\li $q = d_k / A_1[k - 2]$; $r = d_k \bmod A_1[k - 2]$
\li $ST[q] \gets A_2[k - 2]$
\li \For $j \gets 0 \To q-1$
\li \Do $ST[j] \gets A_1[k - 2] + A_2[k - 2]$ \End
\li \For $j \gets q+1 \To B$
\li \Do $ST[j] \gets r + A_2[k-2]$ \End
\li \Return $ST$
\end{codebox}
\end{algorithm}

In this section, we use $N$ to represent the total number of elements and $B$ to represent the number of elements that can fit into a cache line. Binary search on a sorted list of elements laid out as a linear array in the memory incurs $O(log_2N)$ lookups when the search is unsuccessful. This cost can be reduced to $O(log_BN)$ by representing the data using alternative layouts. Cache-oblivious and cache-aware search trees are two prominent examples of such layouts. While the cache-aware layout works by relying on the knowledge of the size of the cache line, cache-oblivious layout works without this knowledge and hence the name. Note that all the search trees we talk about in this paper are static in nature, in that they are not updated once constructed.

\begin{figure*}[!t]
\subfigure[Sorted List Memory Layout]{
\begin{minipage}[]{0.5\linewidth}
\includegraphics[width=\linewidth]{figures.pdf}
\end{minipage}
}
\subfigure[Cache Aware Memory Layout]{
\begin{minipage}[]{0.5\linewidth}
\includegraphics[width=\linewidth]{caware.pdf}
\end{minipage}
}
\caption{Search trees for a sorted list of 26 elements with contents
  $[1,2,\ldots,26]$ in a memory layout of sorted list (left) and cache
  aware layout (right).}
\label{fig-layout}
\end{figure*}

Cache-oblivious data structures and algorithms use optimum number of memory transfers without the knowledge of organization and parameters of the memory \cite{frigo1999cache}. The algorithm for constructing a cache-oblivious search tree from a binary search tree works as follows. The binary search tree is partitioned at half of its height. This process results in a top subtree and $O(\sqrt{N})$ bottom subtrees, each of size $O(\sqrt{N})$. The partitioning process continues recursively on each subtree until we end up with subtrees which can fit into one or two cache lines. It is common to layout subtrees in level order starting from the topmost level, although this is not a requirement. The subtrees within a level ar laid out in left to right order. As the algorithms for constructing and accessing the cache-oblivious search tree are described in detail by Ronn \cite{ronn2003cache}, we do not repeat them in this paper for brevity. An unsuccessful search in the cache-oblivious tree incurs $4 \times log_BN$ accesses in the worst case.

A cache-aware search tree is organized as a $(B+1)$-ary tree, with each node in the tree having $(B+1)$ children in a complete tree. An example cache-aware search tree and its binary counterpart are shown in Figure \ref{fig-layout}(b) and \ref{fig-layout}(a) respectively for N=26, and B=2. The number of levels in the tree is $log_{(B+1)}{(N+1)}$. An unsuccessful search in such a tree incurs $log_BN$ accesses in the worst case. This is a $log_2B$ factor improvement when compared to an unsuccessful search in a binary tree. A node in the cache-ware tree consists of $B$ elements, which among them define $(B+1)$ ranges. $(B+1)$ subtrees of this node are arranged such that, the elements in a subtree fall in the corresponding range. In  Figure \ref{fig-layout}(b), the node containing elements $\{9,18\}$ has three subtrees. The elements in these subtrees are $<9$, $>9\&<18$, and $>18$ respectively. Search in a cache-aware search tree works analogous to how it works in a binary search tree, with the difference that the former incurs a factor of $log_2B$ less accesses than the latter.

The algorithm for constructing a cache-aware search tree is more involved, especially when the tree is incomplete. Although cache-aware search tree has been known more than a decade, we were not able to find an algorithm for its construction in the literature. Therefore, we developed an algorithm for  constructing cache-aware search tree and present it in Algorithm \ref{alg:caware}. For the sake of brevity, we only provide a high level description of this algorithm. The construction starts from the top most level and proceeds to the bottom most level. At a particular level, nodes are laid out left to right in the memory analogous to the cache-oblivious case. While processing a node $x$, the algorithm computes the sizes of each of its $B+1$ subtrees, and thus determines the $B$ elements of $x$. The start and end indices of each of the $B+1$ subtrees are calculated and temporarily stored in the corresponding location. The algorithm, then, proceeds to process the child nodes of $x$. The algorithm works on all nodes at a particular level before proceeding to work on a level below it.


\section{Methodology} \label{sec_Methodology}
Shah et al. \cite{shah2012parallel} used their parallel error correction framework to parallelize the Reptile \cite{yang2010reptile}, a spectrum based error correction algorithm. We also use Parallel Reptile (PReptile) to demonstrate  the performance benefits of using  memory-access efficient search trees. While the higher level working of Reptile is similar to that of other kmer-spectrum based algorithms, we describe one detail which sets up the context for interpreting the results. Reptile constructs and make use of two different spectrums -- kmer-spectrum and tile-spectrum. A tile is formed by concatenating two kmers and  is of length $2k$. The use of tile-spectrum in addition to the kmer-spectrum allows Reptile to better resolve ambiguities during error correction, thus enabling it to achieve better quality of results. The use of PReptile as a representative of the kmer-spectrum based methods for evaluation in this paper implies that two different searches are performed during the error correction phase.

We used the same real datasets as Shah et al. \cite{shah2012parallel} for evaluation, with the following exception. We did not consider dataset D1 used by them due to its small size. The remaining datasets are listed in Table \ref{tab:datasets} and are available from the NCBI Short Read Archive. Datasets D3A (SRX023452), D3B (SRX001651) and D3C (SRX001652) are combined into a single dataset D3, having 98.2 million reads. D2 (SRR034501\_1) and D3 are used in all our experiments. We used the same parameters used by Shah et al. \cite{shah2012parallel} and therefore, we did not evaluate the quality of error correction. We evaluated the runtime performance of error correction for datasets D2 and D3 using three different layouts - binary, cache-oblivious, and cache-aware search trees.

\begin{table}[htb]
\caption{\label{tab:datasets}Datasets for experimental evaluation}
\centering
\begin{tabular}{llrrr}
\hline
Dataset & Genome          & No. Reads  & Length       & Coverage \\
        &                 & (millions) & (bp) &          \\
\hline
D2      & \emph{E.Coli}   & 8.9        & 101          & 193x     \\
D3A     & \emph{Droso. M} & 37.9       & 95           & 30x      \\
D3B     & \emph{Droso. M} & 41.5       & 35           & 12x      \\
D3C     & \emph{Droso. M} & 18.8       & 75           & 12x      \\
\hline
\end{tabular}
\end{table}

All experiments were run on a cluster with InfiniBand interconnect. Each node has two  2.0 GHz 8-Core Intel Xeon E5 2650 processors with 128 GB of main memory, running RedHat Enterprise Linux.  We used up to 64 nodes in the system. Each Intel Xeon E5 2650 processor has a total of 8 cores, which share a 20MB last level cache. Except for the runs with 2, 4 and 8 processes, we allocated 16 MPI processes per node. Further, the cache line size on this processor is 64 bytes across all cache levels. We used this information while constructing the cache-aware layouts for kmer-spectrum and tile-spectrum. In our implementation, kmers and tiles are represented as 4 byte and 8 byte unsigned integers respectively. We used up to 64 nodes in the system, translating into runs using up to 1024 MPI processes.


\section{Results} \label{sec_Results}
The results we obtained by using the cache-oblivious and cache-aware layouts for datasets D2 and D3 are shown in Tables \ref{d2h2results} and \ref{d3h2results} respectively. While presenting the runtime for error correction using cache-oblivious and cache-aware search trees, we include the time taken for constructing the corresponding trees. In order to replace invalid k-mers of an erroneous read, queries are placed in search of valid k-mers by enumerating all possible k-mers at a hamming distance of 1 or 2 (h=2). The kmer-spectrum construction time remains the same for binary, cache-oblivious, and cache-aware search trees. Moreover, we can see that the spectrum construction time is a small fraction of the total time. Therefore, the speedup observed in case of error correction time directly translates to the speedup of total time. For dataset D3, we could not get the results using 64 processes owing to the long runtime.

In case of both datasets, the speedup reported by using cache-oblivious and cache-aware search trees is relative to binary search tree. The maximum speedup is obtained for D2 using cache-aware search tree. The speedup obtained using both search trees reduces slightly as the number of processors is increased. This is because as the number of processes increases, the number of reads assigned per process reduces. Despite this phenomenon, we expect that better speedup can be obtained for larger datasets, such as those generated for humans and plants. As a reference, we provide the sizes of the kmer- and tile- spectrums for both datasets D2 and D3 in Table \ref{tab:spectrum}. The speedup obtained using cache-aware search tree is $>2$x for all scenarios. Since the cache-aware layout is able to leverage the knowledge of the cache line size, it performs better than the cache-oblivious layout.

\begin{table}[htb]
\caption{\label{tab:spectrum}$k$mer spectrum and tile spectrum size}
\centering
\begin{tabular}{lrr}
\hline
Dataset & Kmer Table Size & Tile Table Size\\
\hline
D2 & 1699218 & 8830856\\
D3 & 6092949 & 240045877\\
\hline
\end{tabular}
\end{table}

\begin{table*}[ht]
\normalsize
\renewcommand{\arraystretch}{1.3}
%\begin{minipage}{0.5\textwidth}
\caption{Run-time performance of Parallel Reptile for dataset D2 with $h=2$}
\label{d2h2results}
\centering
\begin{tabular}{r|r|r|rr|rr}
\hline
        & kmer-Spectrum      & Error          & \multicolumn{2}{c|}{Cache Aware} & \multicolumn{2}{c}{Cache Oblivious}  \\
\cline{4-7}
Procs   & Construction    & Correction     & Error Correction & Speedup & Error Correction & Speedup \\
        & Time (seconds)  & Time (seconds) & Time (seconds)   &         & Time (seconds)   &         \\
\hline
64  & 4.79  & 5820.63 & 2430.41 & 2.39 & 3430.27 & 1.69 \\
128 & 4.06  & 2868.85 & 1315.91 & 2.18 & 1819.92 & 1.57 \\
256 & 6.19  & 1627.04 & 740.60  & 2.19 & 1042.59 & 1.56 \\
512 & 13.06 & 835.94  & 380.16  & 2.19 & 537.66  & 1.55 \\
\hline
\end{tabular}
%\end{minipage}
\end{table*}

\begin{table*}[ht]
\normalsize
\renewcommand{\arraystretch}{1.3}
%\begin{minipage}{0.5\textwidth}
\caption{Run-time performance of Parallel Reptile for dataset D3 with $h=2$}
\label{d3h2results}
\centering
\begin{tabular}{r|r|r|rr|rr}
\hline
        & kmer-Spectrum      & Error          & \multicolumn{2}{c|}{Cache Aware} & \multicolumn{2}{c}{Cache Oblivious}  \\
\cline{4-7}
Procs   & Construction    & Correction     & Error Correction & Speedup & Error Correction & Speedup \\
        & Time (seconds)  & Time (seconds) & Time (seconds)   &         & Time (seconds)   &         \\
\hline
256  & 27.49 & 5816.80 & 2633.96 & 2.21 & 3600.86 & 1.61 \\
512  & 33.04 & 3503.59 & 1611.09 & 2.17 & 2184.29 & 1.60 \\
1024 & 53.40 & 2156.35 & 1071.46 & 2.01 & 1313.32 & 1.64 \\
\hline
\end{tabular}
%\end{minipage}
\end{table*}


\section{Discussion} \label{sec_Discussion}
In this section, we analyze the reasons for the discrepancy between the expected and actual performance of the cache-oblivious and cache-aware search trees. Many of them are common between the two layouts and are as follows. First, memory-access efficient layouts leverage on spatial locality for performance. The key insight behind these layouts is to bring those elements, which are accessed consecutively in a binary search tree, together in a spatial sense. However, as the topmost levels of the binary search tree are accessed repeatedly, they benefit from temporal locality. We expect that approximately the elements in top 10 levels of the tree experience cache hits due to temporal locality. Note that the elements in these levels account for about 1K lines in a cache which can hold about 32K lines at a time. The second factor is related to the characteristic that not all lookups will be unsuccessful, and those lookups that are successful can hit at various levels in the tree. Finally, elements in the last $log_2B$ levels of the binary tree benefit from spatial locality as well. The three factors outlined above account for the difference between the expected and actual performance of the memory-access efficient layouts.

In addition, cache-oblivious layout suffers from two disadvantages, which are unique to it. First, some of the lowest level subtrees in the cache-oblivious layout can suffer from bad alignment resulting in them staggering over two cache lines. Second, the lack of knowledge of the cache line size affects this layout. These two factors are primarily responsible for the slower speed up of cache-oblivious layout when compared to the cache-aware layout. However, incomplete cache-oblivious search trees experience a slight advantage in our implementation due to the manner in which we handle them. Specifically, we handle an incomplete cache-oblivious search tree as a collection of multiple complete cache-oblivious search trees of successively smaller sizes, to manage complexity.


\section{Conclusion} \label{sec_Conclusion}
In this work, we improved the performance of the error correction step in a previously proposed parallel algorithm for spectrum-based error correction. We accomplished this by using the cache-oblivious and cache-aware search trees in place of binary search trees. As the former two layouts are optimized for memory accesses, we are able to achieve a speedup of up to 2.4x. In addition, we provided a rigorous analysis of the source of discrepancy between the expected and actual performance of these layouts. 

While developing parallel algorithms, significant emphasis is placed on reducing communication as it is considered to be expensive. In a similar manner, importance needs to be given to optimizing memory-accesses while developing algorithms. This is especially important in case of modern systems in which a memory access is about a few hundred times more expensive than the cost of computation. In this spirit, we adopted the previously proposed memory-access efficient search trees to significantly improve the performance of a previously proposed error correction algorithm. We hope that our work will spur a renewed interest in embracing such memory-access efficient data structures and algorithms for various other problems.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}


\end{document}